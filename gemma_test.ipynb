{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER_ELISEY\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from trl import setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_source_model = \"C:\\\\Users\\\\USER_ELISEY\\\\gemma\"\n",
    "path_to_finetuned_model = \"C:\\\\Users\\\\USER_ELISEY\\\\hakaton\\\\russia_chad\\\\checkpoint-500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "#     model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "#     model_name = \"AnatoliiPotapov/T-lite-instruct-0.1\"\n",
    "    model_name = \"google/gemma-2-9b-it\"\n",
    "    dataset_name = \"ruslanmv/ai-medical-chatbot\"\n",
    "    new_model = \"llama-3.1-8b-chat-doctor\"\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "casual_model = AutoModelForCausalLM.from_pretrained(\n",
    "    path_to_source_model,\n",
    "    quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "tokenizer = tokenizer = AutoTokenizer.from_pretrained(path_to_source_model)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.padding_token = '<|pad_token|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "casual_model, tokenizer = setup_chat_format(casual_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.04s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    path_to_finetuned_model,\n",
    "    quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "tokenizer = tokenizer = AutoTokenizer.from_pretrained(path_to_finetuned_model)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.padding_token = '<|pad_token|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, prompt):\n",
    "    chat = [\n",
    "        { \"role\": \"user\", \"content\": prompt },\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "\n",
    "    return(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"in what year did Alexander Nevsky go to the Horde?\"\n",
    "q2 = \"Who is Stalin son?\"\n",
    "q3 = \"Кто такой Брежнев?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nin what year did Alexander Nevsky go to the Horde?<|im_end|>\\n<|im_start|>assistant\\n<end_of_turn>\\n<start_of_turn><start_of_turn><eos>\\n<eos>\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nWho is Stalin son?<|im_end|>\\n<|im_start|>assistant\\n<end_of_turn>\\n<end_of_turn><eos>\\n<eos>\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nКто такой Брежнев?<|im_end|>\\n<|im_start|>assistant\\n<end_of_turn>\\n<start_of_turn><start_of_turn><eos>\\n<end_of_turn><eos>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<eos>\\n<end_of_turn>\\n<eos>\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Free gpu memory\n",
    "# import numba\n",
    "# numba.cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "in what year did Alexander Nevsky go to the Horde?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn><eos><eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn><eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn><eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn><eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn><eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(casual_model, q1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nWho is Stalin son?<|im_end|>\\n<|im_start|>assistant\\n<end_of_turn><end_of_turn><eos><end_of_turn><eos><end_of_turn>.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(casual_model, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nКто такой Брежнев?<|im_end|>\\n<|im_start|>assistant\\n<end_of_turn>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<end_of_turn><eos>\\n\\n\\n\\n<eos>\\n<end_of_turn><eos>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n<end_of_turn>\\n<eos>\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(casual_model, q3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
