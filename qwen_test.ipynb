{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER_ELISEY\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from trl import setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_source_model = \"C:\\\\Users\\\\USER_ELISEY\\\\qwen2.5_1.5\"\n",
    "path_to_source_model = \"C:\\\\Users\\\\USER_ELISEY\\\\hakaton\\\\russia_chad_1.5_no\\\\final\"\n",
    "path_to_finetuned_model = \"C:\\\\Users\\\\USER_ELISEY\\\\hakaton\\\\russia_chad_1.5_r2\\\\final\"\n",
    "path_to_still_model = \"C:\\\\Users\\\\USER_ELISEY\\\\qwen2.5_1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float32,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "casual_model = AutoModelForCausalLM.from_pretrained(\n",
    "    path_to_source_model,\n",
    "#     device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "tokenizer = tokenizer = AutoTokenizer.from_pretrained(path_to_source_model)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.padding_token = '<|pad_token|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "casual_model, tokenizer = setup_chat_format(casual_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    path_to_finetuned_model,\n",
    "    #quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "tokenizer = tokenizer = AutoTokenizer.from_pretrained(path_to_finetuned_model)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.padding_token = '<|pad_token|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "still_model = AutoModelForCausalLM.from_pretrained(\n",
    "    path_to_still_model,\n",
    "    #quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "tokenizer = tokenizer = AutoTokenizer.from_pretrained(path_to_still_model)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.padding_token = '<|pad_token|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "still_model, tokenizer = setup_chat_format(still_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, prompt):\n",
    "    chat = [\n",
    "        { \"role\": \"user\", \"content\": prompt },\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "\n",
    "    return(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"Кто такой Александр Невский?\"\n",
    "q2 = \"Что такое карибский кризис?\"\n",
    "q3 = \"Выведи a+b  на питоне\"\n",
    "q4 = \"Кто ты такая?\"\n",
    "q5 = \"Кто такой Сталин?\"\n",
    "q6 = \"Кто такой Путин?\"\n",
    "q7 = \"Кто такой Григорий Лепс?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Free gpu memory\n",
    "# import numba\n",
    "# numba.cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Кто такой Александр Невский?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Александр Васильевич Невский (27 декабря 1340, Селигерь — 6 июня 1409, Великий Новгород) — полководец и правителе Руси, один из основателей Москвы, первый князь Московского. Победитель в битве при Луге (1380). Войска его несли сражения на территории современных России и Украины. Впервые был принят князем Киевским, а затем и Российским государством. Впоследствии стал одним из первых русских князей-военачальников\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(casual_model, q1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Что такое карибский кризис?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Карibbean crisis — период в экономике Карибского континента, который начался в 1973 году и завершился в 1985 году. Сначала он привёл к серьёзным потерям для многих стран региона, а затем привёл к устойчивому экономическому развитию и значительной инфляции. В то же время в течение этого периода были произведены важные политические изменения в ряде стран региона, что вызвало значительное влияние на экономику региона.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(casual_model, q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Выведи a+b  на питоне<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Создайте переменную `a`, например, `a = 5` и переменную `b`, например, `b = 3`. Затем выведите сумму этих двух чисел с помощью оператора сложения `+`.\n",
      "\n",
      "```python\n",
      "a = 5\n",
      "b = 3\n",
      "\n",
      "result = a + b\n",
      "print(result)\n",
      "```\n",
      "\n",
      "Результат выполнения: 8<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(casual_model, q3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Кто такой Александр Невский?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Александр Невский - это имя, но не человек. Это фамилия, которая передаётся по наследству от отца к сыну или другим потомкам.\n",
      "\n",
      "Фамилии обычно вводят родители новорожденного, а затем их дети могут изменять свою фамилию после смерти родителей, если они хотят сохранить семейное прозвище или просто предпочитают другую запись своей личности. В этом случае фамилия становится постоянной и часто используется для обозначения человека, даже если его имя изменилось.\n",
      "\n",
      "Так что, как правило, \"Невский\" является фамилией,\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(still_model, q1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Что такое карибский кризис?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Карибский кризис - это термин, который часто используется для обозначения сложных экономических и политических ситуаций в регионе Карибского бассейна. Вот основные аспекты этого кризиса:\n",
      "\n",
      "1. Основные элементы:\n",
      "   - Нарушение финансовой стабильности стран- debtor\n",
      "   - Проблемы с выплатами внешнего долга\n",
      "   - Утрата доверия между creditors и должниками\n",
      "\n",
      "2. Видимые признаки:\n",
      "   - Снижение уровня инвестиций\n",
      "   - Детоксификация банковских систем\n",
      "   - Рост процент\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(still_model, q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Выведи a+b  на питоне<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Вы можете вывести сумму двух чисел `a` и `b` на Python следующим образом:\n",
      "\n",
      "```python\n",
      "a = 5\n",
      "b = 3\n",
      "\n",
      "result = a + b\n",
      "print(result)\n",
      "```\n",
      "\n",
      "Этот код сначала объявляет переменные `a` и `b`, затем их добавляет и сохраняет результат в переменной `result`. Затем выводится значение этой переменной. В данном случае, это будет равно 8.\n",
      "\n",
      "Если вам нужно выполнить эту операцию сразу после объявления переменных без использования дополнительного переменной для хранения результата, то можно сделать так:\n",
      "\n",
      "```python\n",
      "a = 5\n",
      "b = 3\n",
      "print(a\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(still_model, q3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Кто ты такая?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Я искусственный интеллект, созданный компанией Anthropic. У меня нет личности или имени, я просто программа, созданная для общения и помощи пользователям. Могу помочь с разными вопросами!<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(still_model, q4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Кто такой Григорий Лепс?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Григорий Лепс - это имя, которое можно встретить в различных контекстах:\n",
      "\n",
      "1. Фамилия: Григорий Лепс может быть фамилией человека.\n",
      "\n",
      "2. Название: Это название для музыкального произведения \"Лепс\" (фортепианный концерт).\n",
      "\n",
      "3. Имя: В некоторых странах и культурных кругах существует имя Григорий Лепс.\n",
      "\n",
      "4. Отчество: Может использоваться как отчество в некоторых случаях.\n",
      "\n",
      "5. Пseudonym: Возможно, это псевдоним или инициалы известной личности.\n",
      "\n",
      "6. Место рождения: Существ\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(still_model, q7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
