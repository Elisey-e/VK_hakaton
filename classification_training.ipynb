{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER_ELISEY\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer#, TFBertForSequenceClassification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.metrics import BinaryAccuracy\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "miracl_dataset = load_dataset(\"miracl/miracl\", \"ru\")\n",
    "miracl_corpus = load_dataset(\"miracl/miracl-corpus\", \"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    dev: Dataset({\n",
      "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
      "        num_rows: 1252\n",
      "    })\n",
      "    testB: Dataset({\n",
      "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
      "        num_rows: 718\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
      "        num_rows: 4683\n",
      "    })\n",
      "    testA: Dataset({\n",
      "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
      "        num_rows: 911\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['docid', 'title', 'text'],\n",
      "        num_rows: 9543918\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(miracl_dataset)\n",
    "print(miracl_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = {row[\"docid\"]: row for row in miracl_corpus[\"train\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"dev\"]:  # Use both train and dev splits\n",
    "    for example in miracl_dataset[split]:\n",
    "        query = example[\"query\"]\n",
    "        # Add positive (relevant) pairs\n",
    "        for pos_passage in example[\"positive_passages\"]:\n",
    "            pos_docid = pos_passage[\"docid\"]  # Extract docid from the dictionary\n",
    "            if pos_docid in corpus_dict:\n",
    "                qa_pairs.append((query, corpus_dict[pos_docid][\"text\"]))\n",
    "                labels.append(1)  # Relevant\n",
    "        # Add negative (irrelevant) pairs\n",
    "        for neg_passage in example[\"negative_passages\"]:\n",
    "            neg_docid = neg_passage[\"docid\"]  # Extract docid from the dictionary\n",
    "            if neg_docid in corpus_dict:\n",
    "                qa_pairs.append((query, corpus_dict[neg_docid][\"text\"]))\n",
    "                labels.append(0)  # Irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = pd.DataFrame(qa_pairs, columns=[\"query\", \"text\"])\n",
    "qa_df[\"label\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Когда был спущен на воду первый миноносец «Спо...</td>\n",
       "      <td>Зачислен в списки ВМФ СССР 19 августа 1952 год...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Когда был спущен на воду первый миноносец «Спо...</td>\n",
       "      <td>Стерегу́щий — русский миноносец типа «Сокол». ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Когда был спущен на воду первый миноносец «Спо...</td>\n",
       "      <td>Эскадренный миноносец заложен в 1900 году на Н...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Когда был спущен на воду первый миноносец «Спо...</td>\n",
       "      <td>10 октября 1937 года эсминец был спущен на вод...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Когда был спущен на воду первый миноносец «Спо...</td>\n",
       "      <td>В 1901 году миноносец «Бодрый» был зачислен в ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  Когда был спущен на воду первый миноносец «Спо...   \n",
       "1  Когда был спущен на воду первый миноносец «Спо...   \n",
       "2  Когда был спущен на воду первый миноносец «Спо...   \n",
       "3  Когда был спущен на воду первый миноносец «Спо...   \n",
       "4  Когда был спущен на воду первый миноносец «Спо...   \n",
       "\n",
       "                                                text  label  \n",
       "0  Зачислен в списки ВМФ СССР 19 августа 1952 год...      1  \n",
       "1  Стерегу́щий — русский миноносец типа «Сокол». ...      0  \n",
       "2  Эскадренный миноносец заложен в 1900 году на Н...      0  \n",
       "3  10 октября 1937 года эсминец был спущен на вод...      0  \n",
       "4  В 1901 году миноносец «Бодрый» был зачислен в ...      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    inputs = data[\"query\"] + \" [SEP] \" + data[\"text\"]\n",
    "    model_inputs = tokenizer(inputs.tolist(), max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"np\")\n",
    "    labels = np.array(data[\"label\"])\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = preprocess(qa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = preprocessed_data[\"input_ids\"]\n",
    "attention_mask = preprocessed_data[\"attention_mask\"]\n",
    "labels = preprocessed_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, test_input_ids, train_attention_mask, test_attention_mask, train_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_mask, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TFBertForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFBertForSequenceClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-multilingual-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TFBertForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "#model = TFBertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "# from datasets import Dataset\n",
    "\n",
    "# # Load the tokenizer and model for PyTorch\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "\n",
    "# # Tokenize the dataset and prepare it for PyTorch\n",
    "# def tokenize_data(examples):\n",
    "#     return tokenizer(examples[\"query\"], examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# # Create the Dataset from your question-answer pairs and labels\n",
    "# qa_df[\"label\"] = qa_df[\"label\"].astype(int)  # Ensure labels are integers for PyTorch\n",
    "# dataset = Dataset.from_pandas(qa_df)\n",
    "\n",
    "# # Apply tokenization\n",
    "# tokenized_dataset = dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "# # Split into train and test datasets\n",
    "# train_dataset = tokenized_dataset.train_test_split(test_size=0.2)[\"train\"]\n",
    "# test_dataset = tokenized_dataset.train_test_split(test_size=0.2)[\"test\"]\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "#     save_strategy=\"epoch\",        # Save at the end of each epoch\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "# )\n",
    "\n",
    "# # Define the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Evaluate the model\n",
    "# results = trainer.evaluate()\n",
    "# print(\"Test Loss, Test Accuracy:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "\n",
    "# Tokenize the dataset and prepare it for PyTorch\n",
    "def tokenize_data(examples):\n",
    "    return tokenizer(examples[\"query\"], examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Create the Dataset from your question-answer pairs and labels\n",
    "qa_df[\"label\"] = qa_df[\"label\"].astype(int)  # Ensure labels are integers for PyTorch\n",
    "dataset = Dataset.from_pandas(qa_df)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "# Предполагаем, что `tokenized_dataset` уже создан\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "results = []\n",
    "\n",
    "# Подготовка аргументов для тренировки\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,  # Установим больше эпох, чтобы дать возможность ранней остановки\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,  # Загрузка лучшей модели в конце\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Метки для стратифицированного разбиения\n",
    "labels = np.array(tokenized_dataset['label'])\n",
    "\n",
    "# Кросс-валидация\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "    print(f\"Training fold {fold + 1}/5\")\n",
    "    \n",
    "    # Разделение датасета на обучающую и тестовую выборки для текущего фолда\n",
    "    train_dataset = tokenized_dataset.select(train_idx)\n",
    "    test_dataset = tokenized_dataset.select(test_idx)\n",
    "\n",
    "    # Создание модели для текущего фолда\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "\n",
    "    # Определение тренера с ранней остановкой\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Ранняя остановка после 3 эпох без улучшений\n",
    "    )\n",
    "    \n",
    "    # Обучение модели\n",
    "    trainer.train()\n",
    "\n",
    "    # Оценка модели\n",
    "    fold_results = trainer.evaluate()\n",
    "    print(f\"Fold {fold + 1} results:\", fold_results)\n",
    "    results.append(fold_results[\"eval_loss\"])\n",
    "\n",
    "# Средняя точность по всем фолдам\n",
    "print(\"Average loss across all folds\", np.mean(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./relevance_10_tokenizer\\\\tokenizer_config.json',\n",
       " './relevance_10_tokenizer\\\\special_tokens_map.json',\n",
       " './relevance_10_tokenizer\\\\vocab.txt',\n",
       " './relevance_10_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./relevance_10_model\")\n",
    "tokenizer.save_pretrained(\"./relevance_10_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"./relevance_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./relevance_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.5901882648468018}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Set up a classification pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Combine question and answer with a separator\n",
    "question = \"Работает ли наша модель?\"\n",
    "answer = \"Пожалуй.\"\n",
    "combined_input = question + \" [SEP] \" + answer  # or simply question + \" \" + answer\n",
    "\n",
    "# Run inference on the combined question-answer pair\n",
    "result = classifier(combined_input)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
